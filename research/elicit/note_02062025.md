Recent research explores the application of random matrix theory (RMT) and tensor networks in machine learning and related fields. Tensor networks, inspired by quantum many-body physics, have shown promise in machine learning tasks (Convy et al., 2021; Liu et al., 2017). These networks can encode image classes into quantum many-body states, allowing for the study of quantum features like entanglement and fidelity (Liu et al., 2017). Mutual information scaling in classical data can guide the design of tensor networks for specific learning tasks (Convy et al., 2021). RMT has been applied to analyze deep neural networks, providing insights into their convergence and learning speed (Ge et al., 2021). Additionally, tensor network architectures have demonstrated strong generalizability and efficiency in predicting materials properties, with their performance linked to complex entanglement in the trained networks (Sommer & Dunham, 2022). These studies highlight the growing intersection of quantum-inspired methods and machine learning techniques.

Recent research has explored the use of matrix product operators (MPOs) for efficient representation and analysis of quantum systems. Cygorek & Gauger (2024) demonstrated that inner bonds of process tensor MPOs have physical meaning, representing environment subspaces that significantly influence open quantum system dynamics. Hino & Kurashige (2024) developed a method to compress many-body potential energy surfaces into grid-based MPOs, achieving significant compression without loss of accuracy. Cui et al. (2015) introduced a variational MPO method for finding steady states of dissipative quantum chains, allowing faster convergence for states with small bond dimensions. Qin et al. (2023) established theoretical guarantees for quantum state tomography of MPO states, showing that only a polynomial number of state copies is required for bounded recovery error. These studies collectively demonstrate the versatility and efficiency of MPOs in representing and analyzing complex quantum systems across various applications.

Recent research has explored the application of tensor-train decomposition to large language models (LLMs) for improved efficiency and performance. Tensor-Train Language Models (TTLM) represent sentences in an exponential space constructed by the tensor product of words, outperforming vanilla RNNs with low-scale hidden units (Su et al., 2024). TensorGPT, a training-free compression approach, applies tensor-train decomposition to token embeddings, achieving significant parameter reduction and compression ratios in GPT family models while maintaining comparable performance (Xu et al., 2023). This method can compress the embedding layer by up to 38.40 times, sometimes even improving performance (Xu et al., 2023). The tensor-train approach has also been successfully applied to video classification tasks, where it addresses the challenge of high-dimensional inputs in Recurrent Neural Networks, achieving competitive performance with state-of-the-art models despite a much simpler architecture (Yang et al., 2017).
