Recent research has focused on improving tensor decomposition methods, particularly for high-order and large-scale tensors. The Tensor Train (TT) decomposition has gained attention due to its linear storage cost with tensor order (Yassine Zniyed et al., 2020). Advancements include the TT-HSVD algorithm, which offers a more efficient and parallelizable alternative to TT-SVD (Yassine Zniyed et al., 2020), and adaptive rank selection methods for Tensor Ring decomposition to optimize storage costs (F. Sedighin et al., 2021). For large-scale matrices, SVD algorithms based on low-rank TT decomposition have been developed, demonstrating logarithmic computational complexity with matrix size for bounded TT-ranks (Namgil Lee & Cichocki, 2014). Additionally, the TT-UTV method has been proposed as a computationally efficient alternative to TT-SVD, utilizing rank-revealing UTV decomposition (Yuchao Wang et al., 2025). These advancements aim to improve the efficiency and applicability of tensor decomposition techniques for big data and high-dimensional problems.


 Description of the TT-HSVD algorithm. In this section, the TT-
HSVD algorithm is presented, with the aim to derive the TT-cores in a parallel hi-
erarchical way. The main diﬀerence between the TT-SVD and TT-HSVD algorithms
lies in the initial matrix unfolding to be processed by the SVD, and the reshaping
strategy, i.e. the way to reshape the SVD factors (Ud,Vd), at each step. Fig. 5 illus-
trates the proposed strategy by means of the graph-based representation of the TT
decomposition of a 4-order tensor. This figure is to be compared with Fig. 4. With
¯
the TT-HSVD algorithm, for an a priori chosen index
D∈{1,...,D}, the first matrix
unfolding X(
¯
D) is of size (I1···I¯
D ) ×(I¯
D+1···ID ) instead of I1 ×(I2···ID ) as for
the TT-SVD algorithm, which leads to a more rectangular matrix. Its R¯
D -truncated
SVD provides two factors U¯
D and V¯
D of size (I1···I¯
D )×R¯
D and R¯
D ×(I¯
D+1···ID ),
respectively. These two factors are now reshaped in parallel, which constitutes the
main diﬀerence with the TT-SVD algorithm for which only a single reshaping oper-
ation is applied to V1. This processing is repeated after each SVD computation, as
illustrated in Fig. 5 for a 4-order tensor.
Generally speaking, the choice of the best reshaping strategy, i.e., the choice of the
¯
index
D, is depending on an a priori physical knowledge related to each application
8 TT-HSVD
Fig. 4. TT-SVD applied to a 4-order tensor.
[36, 6], as for instance, in bio-medical signal analysis, or in wireless communications
[14, 66, 20]. In Section 7, this best choice is discussed in terms of algorithmic com-
plexity. To illustrate this choice, two reshaping strategies are considered in Fig. 6
(left) and Fig. 6 (right) for computing the TT decomposition of a 8-order tensor with
the TT-HSVD algorithm. More precisely, Fig. 6 (left) corresponds to a balanced
unfolding with I¯
D = I4, while Fig. 6 (right) corresponds to an unbalanced unfolding
with I¯
D = I3. From this simple example, one can conclude that this graph-based rep-
resentation is not illustrative, which motivated the new graph-based representation
using patterns, introduced in the next section.
4.3. Graph-based representation with patterns. A group of algorithmic
instructions can be identified as recurrent in the TT-SVD and TT-HSVD algorithms.
Such a group will be called a pattern. The concept of pattern is introduced in this
work as a useful graphical tool to model the processing steps of TT decomposition
algorithms in an illustrative way. It also facilitates understanding of the main steps
of the TT-HSVD algorithm, although it can also be used to describe any algorithm.
Three types of pattern are now described.
4.3.1. Splitting/Splitting pattern.
The Splitting/Splitting pattern takes as input any matrix unfolding X(
¯
D) of size
¯
R¯
Df (I¯
Df +1I¯
Df +2···I¯
D )×(I¯
D+1I¯
D+2···I¯
Dl )R¯
Dl , where
Df stands for the first index
¯
and
Dl for the last index. This pattern applies the SVD to the input matrix and
generates a matrix U¯
D , of size R¯
Df (I¯
Df +1I¯
Df +2···I¯
D ) ×R¯
D , which contains the
left singular vectors, and a matrix V¯
D , of size R¯
D ×(I¯
D+1I¯
D+2···I¯
Dl )R¯
Dl , equal to
TT-HSVD 9
Fig. 5. TT-HSVD applied to a 4-order tensor.
the product of the diagonal singular values matrix with the matrix composed of the
right singular vectors. These two factors (U¯
D ,V¯
D ) are then reshaped using two new
¯
¯
indices
D′and
D′′. This pattern is represented in Fig. 7. The corresponding graph is
¯
¯
characterized by one input X(
¯
D), two indices
D′and
D′′, and two outputs X(
¯
¯
Df +
D′)
and X(
¯
¯
D+
D′′). This pattern plays the role of data preparation, before generating the
desired TT-cores using other patterns that will be called core generation patterns. One
can notice that this pattern is always used at the top of the TT-HSVD algorithm,
¯
¯
with
Df = 0, and
Dl = D, the tensor order. (See Figs. 5, 6 (left) and 6 (right)).
4.3.2. Mixed patterns: data processing and TT-core generation. The
first mixed pattern, that will be called Splitting/Generation pattern, takes as in-
put any matrix X(
¯
D) of size R¯
Df (I¯
Df +1I¯
Df +2···I¯
D ) ×(I¯
D+1R¯
D+1), and returns a
reshaped matrix and a tensor. It has the same structure as the splitting/splitting
pattern, except that it generates a matrix and a tensor instead of two matrices. This
pattern is represented in Fig. 8 (left). For example, this pattern can be seen in Fig.
6 (right) as the function that takes as input X(2) and generates X(1) and G3. The
second mixed pattern, that will be called Generation/Splitting pattern, takes as input
any matrix X(
¯
D) of size (R¯
D−1I¯
D )×(I¯
D+1I¯
D+2···I¯
Dl )R¯
Dl , and returns a tensor and
a reshaped matrix. This pattern is represented in Fig. 8 (right). For example, this
pattern can be seen in Fig. 6 (right) as the function that takes as input X(6) and
generates G6 and X(7).
4.3.3. TT-core generation pattern. The TT-core generation pattern gener-
ates two core tensors in parallel. It will be called Generation/Generation pattern. It
10 TT-HSVD
Fig. 6. Balanced (left) and Unbalanced (right) TT-HSVD applied to a 8-order tensor.
is represented in Fig. 9. It takes as input X(
¯
D) of size (R¯
D−1I¯
D ) ×(I¯
D+1R¯
D+1) and
returns two core tensors as outputs. For example, this pattern can be recognized in
Fig. 6 (right) as the function that takes as input X(4) and generates G4 and G5.
4.4. Application of the pattern formalism. Note that the TT-SVD algo-
rithm uses one Splitting/Splitting pattern at the beginning and then a sequence of
Generation/Splitting patterns, while the three diﬀerent patterns can be used for the
TT-HSVD algorithm. Figs.10 and 11 illustrate the pattern-based representation of
the TT-HSVD algorithm applied to an 8-order tensor, in the balanced and unbalanced
cases, respectively. These figures are to be compared with Figs. 6 (left) and 6 (right).
